{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 7s 1us/step\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.5177\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.3522\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.2929\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.2632\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.2450\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.2325\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.2234\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.2162\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.2106\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.2059\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1525\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1281\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1136\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1056\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1015\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0992\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0978\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0968\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0962\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13c37834590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos MNIST\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Preprocesamiento de los datos\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Definir la arquitectura del autoencoder\n",
    "input_dim = 784  # Tamaño de la imagen de entrada (28x28 píxeles)\n",
    "encoding_dim = 32  # Dimensión de la capa de codificación\n",
    "\n",
    "input_img = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# Congelar los pesos de las capas\n",
    "autoencoder.layers[1].trainable = False\n",
    "\n",
    "# Compilar y entrenar el autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True)\n",
    "\n",
    "# Descongelar los pesos de las capas\n",
    "autoencoder.layers[1].trainable = True\n",
    "\n",
    "# Compilar y entrenar nuevamente el autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\DELIA\\tensorflow_datasets\\plant_village\\1.0.2...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 111.00 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 76.91 url/s] \n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 58.83 url/s]\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\n",
      "Dl Size...: 100%|██████████| 868032562/868032562 [00:00<00:00, 43410984486.66 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 45.46 url/s]\n",
      "                                                                \r"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No examples were yielded.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# # Cargar el conjunto de datos plant_village\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m dataset, info \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mplant_village\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, shuffle_files\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, with_info\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# # Preprocesamiento de los datos\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# def preprocess_data(item):\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m#     image = item['image']\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# autoencoder.fit(dataset, epochs=10)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:640\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \n\u001b[0;32m    523\u001b[0m \u001b[39m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m dbuilder \u001b[39m=\u001b[39m _fetch_builder(\n\u001b[0;32m    635\u001b[0m     name,\n\u001b[0;32m    636\u001b[0m     data_dir,\n\u001b[0;32m    637\u001b[0m     builder_kwargs,\n\u001b[0;32m    638\u001b[0m     try_gcs,\n\u001b[0;32m    639\u001b[0m )\n\u001b[1;32m--> 640\u001b[0m _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[0;32m    642\u001b[0m \u001b[39mif\u001b[39;00m as_dataset_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m   as_dataset_kwargs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:499\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[0;32m    498\u001b[0m   download_and_prepare_kwargs \u001b[39m=\u001b[39m download_and_prepare_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m--> 499\u001b[0m   dbuilder\u001b[39m.\u001b[39;49mdownload_and_prepare(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:646\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    644\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mread_from_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir)\n\u001b[0;32m    645\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 646\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[0;32m    647\u001b[0m       dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[0;32m    648\u001b[0m       download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m    649\u001b[0m   )\n\u001b[0;32m    651\u001b[0m   \u001b[39m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    652\u001b[0m   \u001b[39m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    653\u001b[0m   \u001b[39m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    654\u001b[0m   \u001b[39m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdownload_size \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1535\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1523\u001b[0m   \u001b[39mfor\u001b[39;00m split_name, generator \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   1524\u001b[0m       split_generators\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1525\u001b[0m       desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating splits...\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1526\u001b[0m       unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m splits\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1527\u001b[0m       leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1528\u001b[0m   ):\n\u001b[0;32m   1529\u001b[0m     filename_template \u001b[39m=\u001b[39m naming\u001b[39m.\u001b[39mShardedFileTemplate(\n\u001b[0;32m   1530\u001b[0m         split\u001b[39m=\u001b[39msplit_name,\n\u001b[0;32m   1531\u001b[0m         dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[0;32m   1532\u001b[0m         data_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path,\n\u001b[0;32m   1533\u001b[0m         filetype_suffix\u001b[39m=\u001b[39mpath_suffix,\n\u001b[0;32m   1534\u001b[0m     )\n\u001b[1;32m-> 1535\u001b[0m     future \u001b[39m=\u001b[39m split_builder\u001b[39m.\u001b[39;49msubmit_split_generation(\n\u001b[0;32m   1536\u001b[0m         split_name\u001b[39m=\u001b[39;49msplit_name,\n\u001b[0;32m   1537\u001b[0m         generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[0;32m   1538\u001b[0m         filename_template\u001b[39m=\u001b[39;49mfilename_template,\n\u001b[0;32m   1539\u001b[0m         disable_shuffling\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mdisable_shuffling,\n\u001b[0;32m   1540\u001b[0m     )\n\u001b[0;32m   1541\u001b[0m     split_info_futures\u001b[39m.\u001b[39mappend(future)\n\u001b[0;32m   1543\u001b[0m \u001b[39m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:341\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[39m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generator, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mIterable):\n\u001b[1;32m--> 341\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_from_generator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuild_kwargs)\n\u001b[0;32m    342\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Otherwise, beam required\u001b[39;00m\n\u001b[0;32m    343\u001b[0m   unknown_generator_type \u001b[39m=\u001b[39m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    344\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInvalid split generator value for split `\u001b[39m\u001b[39m{\u001b[39;00msplit_name\u001b[39m}\u001b[39;00m\u001b[39m`. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    345\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mExpected generator or apache_beam object. Got: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    346\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(generator)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    347\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:418\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    416\u001b[0m     utils\u001b[39m.\u001b[39mreraise(e, prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to encode example:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    417\u001b[0m   writer\u001b[39m.\u001b[39mwrite(key, example)\n\u001b[1;32m--> 418\u001b[0m shard_lengths, total_size \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[0;32m    420\u001b[0m split_info \u001b[39m=\u001b[39m splits_lib\u001b[39m.\u001b[39mSplitInfo(\n\u001b[0;32m    421\u001b[0m     name\u001b[39m=\u001b[39msplit_name,\n\u001b[0;32m    422\u001b[0m     shard_lengths\u001b[39m=\u001b[39mshard_lengths,\n\u001b[0;32m    423\u001b[0m     num_bytes\u001b[39m=\u001b[39mtotal_size,\n\u001b[0;32m    424\u001b[0m     filename_template\u001b[39m=\u001b[39mfilename_template,\n\u001b[0;32m    425\u001b[0m )\n\u001b[0;32m    426\u001b[0m \u001b[39mreturn\u001b[39;00m _SplitInfoFuture(\u001b[39mlambda\u001b[39;00m: split_info)\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\writer.py:243\u001b[0m, in \u001b[0;36mWriter.finalize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Effectively writes examples to the tfrecord files.\"\"\"\u001b[39;00m\n\u001b[0;32m    242\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename_template\u001b[39m.\u001b[39msharded_filepaths_pattern()\n\u001b[1;32m--> 243\u001b[0m shard_specs \u001b[39m=\u001b[39m _get_shard_specs(\n\u001b[0;32m    244\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_examples,\n\u001b[0;32m    245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shuffler\u001b[39m.\u001b[39;49msize,\n\u001b[0;32m    246\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shuffler\u001b[39m.\u001b[39;49mbucket_lengths,\n\u001b[0;32m    247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filename_template,\n\u001b[0;32m    248\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shard_config,\n\u001b[0;32m    249\u001b[0m )\n\u001b[0;32m    250\u001b[0m \u001b[39m# Here we just loop over the examples, and don't use the instructions, just\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[39m# the final number of examples in every shard. Instructions could be used to\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39m# parallelize, but one would need to be careful not to sort buckets twice.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m examples_generator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\n\u001b[0;32m    254\u001b[0m     utils\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shuffler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m     )\n\u001b[0;32m    261\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\writer.py:111\u001b[0m, in \u001b[0;36m_get_shard_specs\u001b[1;34m(num_examples, total_size, bucket_lengths, filename_template, shard_config)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns list of _ShardSpec instances, corresponding to shards to write.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39m  shard_config: the configuration for creating shards.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m num_shards \u001b[39m=\u001b[39m shard_config\u001b[39m.\u001b[39mget_number_shards(total_size, num_examples)\n\u001b[1;32m--> 111\u001b[0m shard_boundaries \u001b[39m=\u001b[39m _get_shard_boundaries(num_examples, num_shards)\n\u001b[0;32m    112\u001b[0m shard_specs \u001b[39m=\u001b[39m []\n\u001b[0;32m    113\u001b[0m bucket_indexes \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(bucket_lengths))]\n",
      "File \u001b[1;32mc:\\Users\\DELIA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\writer.py:142\u001b[0m, in \u001b[0;36m_get_shard_boundaries\u001b[1;34m(num_examples, number_of_shards)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_shard_boundaries\u001b[39m(\n\u001b[0;32m    138\u001b[0m     num_examples: \u001b[39mint\u001b[39m,\n\u001b[0;32m    139\u001b[0m     number_of_shards: \u001b[39mint\u001b[39m,\n\u001b[0;32m    140\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m    141\u001b[0m   \u001b[39mif\u001b[39;00m num_examples \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo examples were yielded.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m   \u001b[39mif\u001b[39;00m num_examples \u001b[39m<\u001b[39m number_of_shards:\n\u001b[0;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnum_examples (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) < number_of_shards (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    146\u001b[0m             num_examples, number_of_shards\n\u001b[0;32m    147\u001b[0m         )\n\u001b[0;32m    148\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: No examples were yielded."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# # Cargar el conjunto de datos plant_village\n",
    "dataset, info = tfds.load('plant_village', split='train', shuffle_files=True, with_info=True)\n",
    "\n",
    "# # Preprocesamiento de los datos\n",
    "# def preprocess_data(item):\n",
    "#     image = item['image']\n",
    "#     image = tf.cast(image, tf.float32) / 255.0\n",
    "#     image = tf.image.resize(image, (128, 128))  # Cambia el tamaño de las imágenes a 128x128 píxeles\n",
    "#     return image, image\n",
    "\n",
    "# dataset = dataset.map(preprocess_data)\n",
    "# dataset = dataset.batch(256)\n",
    "\n",
    "# # Definir la arquitectura del autoencoder\n",
    "# input_shape = (128, 128, 3)  # Tamaño de la imagen de entrada (128x128 píxeles en color)\n",
    "# encoding_dim = 32  # Dimensión de la capa de codificación\n",
    "\n",
    "# input_img = Input(shape=input_shape)\n",
    "# encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# decoded = Dense(input_shape[2], activation='sigmoid')(encoded)\n",
    "\n",
    "# autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# # Congelar los pesos de las capas\n",
    "# autoencoder.layers[1].trainable = False\n",
    "\n",
    "# # Compilar y entrenar el autoencoder\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# autoencoder.fit(dataset, epochs=10)\n",
    "\n",
    "# # Descongelar los pesos de las capas\n",
    "# autoencoder.layers[1].trainable = True\n",
    "\n",
    "# # Compilar y entrenar nuevamente el autoencoder\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# autoencoder.fit(dataset, epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
